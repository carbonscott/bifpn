{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import exploratory tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ml_utils.misc import set_seed, print_layers, get_device\n",
    "from ml_utils.data import Pad, Resize, generate_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class ConvNextV2BackBone(nn.Module):\n",
    "    def __init__(self, pretrained = True):\n",
    "        super().__init__()\n",
    "\n",
    "        model = timm.create_model('convnextv2_atto.fcmae', pretrained=pretrained)\n",
    "\n",
    "        in_channels = 1\n",
    "        out_channels = model.stem[0].out_channels\n",
    "\n",
    "        ave_weight_patch_embd = model.stem[0].weight.data.mean(dim = in_channels, keepdim = True) # [40, 3, 4, 4] -> [40, 1, 4, 4]\n",
    "        model.stem[0] = nn.Conv2d(in_channels, out_channels, kernel_size=(4, 4), stride=(4, 4))\n",
    "        model.stem[0].weight.data = ave_weight_patch_embd\n",
    "        \n",
    "        self.stem  = model.stem\n",
    "        self.stages = model.stages\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input through embeddings\n",
    "        embedding_output = self.stem(x)\n",
    "\n",
    "        # Initialize a list to hold the feature maps from each stage\n",
    "        stage_feature_maps = []\n",
    "\n",
    "        # Manually forward through each stage\n",
    "        hidden_states = embedding_output\n",
    "        for stage in self.stages:\n",
    "            hidden_states = stage(hidden_states)\n",
    "            stage_feature_maps.append(hidden_states)\n",
    "\n",
    "        return embedding_output, stage_feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "model = ConvNextV2BackBone(True).to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bifpn.bifpn        import BiFPN\n",
    "from bifpn.bifpn_config import BiFPNConfig\n",
    "from bifpn.utils_build  import BackboneToBiFPNAdapter, BackboneToBiFPNAdapterConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B, C, H, W = 10, 1, 1920, 1920\n",
      "batch_input = np.random.rand(B, C, H, W)\n",
      "\n",
      "H_unify, W_unify = 1024, 1024\n",
      "resizer = Resize(H_unify, W_unify)\n",
      "\n",
      "batch_input_unify = resizer(batch_input.reshape(B*C, H, W)).reshape(B, C, H_unify, W_unify)\n",
      "batch_input_unify_tensor = torch.from_numpy(batch_input_unify).to(torch.float)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 10, 1, 1920, 1920\n",
    "batch_input = np.random.rand(B, C, H, W)\n",
    "\n",
    "H_unify, W_unify = 1024, 1024\n",
    "resizer = Resize(H_unify, W_unify)\n",
    "\n",
    "batch_input_unify = resizer(batch_input.reshape(B*C, H, W)).reshape(B, C, H_unify, W_unify)\n",
    "batch_input_unify_tensor = torch.from_numpy(batch_input_unify).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_input_unify_tensor_cuda = batch_input_unify_tensor.to(device)\n",
    "    embedding_output, stage_feature_maps = model(batch_input_unify_tensor_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 40, 256, 256])\n",
      "torch.Size([10, 80, 128, 128])\n",
      "torch.Size([10, 160, 64, 64])\n",
      "torch.Size([10, 320, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(stage_feature_maps)):\n",
    "    print(stage_feature_maps[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bifpn_features = 256\n",
    "backbone_output_channels = {\n",
    "        \"layer1\" : 40,\n",
    "        \"layer2\" : 80,\n",
    "        \"layer3\" : 160,\n",
    "        \"layer4\" : 320,\n",
    "}\n",
    "\n",
    "config = BackboneToBiFPNAdapterConfig(num_bifpn_features = num_bifpn_features, backbone_output_channels = backbone_output_channels)\n",
    "backbone_to_bifpn = BackboneToBiFPNAdapter(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bifpn_input_list = backbone_to_bifpn(stage_feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256, 256, 256])\n",
      "torch.Size([10, 256, 128, 128])\n",
      "torch.Size([10, 256, 64, 64])\n",
      "torch.Size([10, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bifpn_input_list)):\n",
    "    print(bifpn_input_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiFPNConfig = BiFPN.get_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiFPNConfig(RELU_INPLACE=False, DOWN_SCALE_FACTOR=0.5, UP_SCALE_FACTOR=2, NUM_BLOCKS=1, NUM_FEATURES=256, NUM_LEVELS=4, BASE_LEVEL=2, BN=BNConfig(EPS=1e-05, MOMENTUM=0.1), FUSION=FusionConfig(EPS=1e-05))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BiFPNConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the BiFPN layer...\n",
    "bifpn = BiFPN(config = BiFPNConfig)\n",
    "bifpn_output_list = bifpn(bifpn_input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256, 256, 256])\n",
      "torch.Size([10, 256, 128, 128])\n",
      "torch.Size([10, 256, 64, 64])\n",
      "torch.Size([10, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bifpn_output_list)):\n",
    "    print(bifpn_output_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict, is_dataclass\n",
    "from typing import List\n",
    "\n",
    "class SegLateralLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_groups, num_layers, base_scale_factor = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enables_upsample = num_layers > 0\n",
    "\n",
    "        # Strange strategy, but...\n",
    "        num_layers = max(num_layers, 1)\n",
    "\n",
    "        # 3x3 convolution with pad 1, group norm and relu...\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels  = (in_channels if idx == 0 else out_channels),\n",
    "                          out_channels = out_channels,\n",
    "                          kernel_size  = 3,\n",
    "                          padding      = 1,),\n",
    "                nn.GroupNorm(num_groups, out_channels),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            for idx in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.base_scale_factor = base_scale_factor\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Conv3x3...\n",
    "            x = layer(x)\n",
    "\n",
    "            # Optional upsampling...\n",
    "            if self.enables_upsample:\n",
    "                x = F.interpolate(x,\n",
    "                                  scale_factor  = self.base_scale_factor,\n",
    "                                  mode          = 'bilinear',\n",
    "                                  align_corners = False)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SegHeadConfig:\n",
    "    UP_SCALE_FACTOR: List[int] = field(\n",
    "        default_factory = lambda : [\n",
    "            ## 2,  # stem\n",
    "            4,  # layer1\n",
    "            8,  # layer2\n",
    "            16, # layer3\n",
    "            32, # layer4\n",
    "        ]\n",
    "    )\n",
    "    NUM_GROUPS           : int  = 32\n",
    "    OUT_CHANNELS         : int  = 256\n",
    "    NUM_CLASSES          : int  = 3\n",
    "    BASE_SCALE_FACTOR    : int  = 2\n",
    "    USES_LEARNED_UPSAMPLE: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "seghead_config = SegHeadConfig()\n",
    "\n",
    "# Create the prediction head...\n",
    "base_scale_factor         = seghead_config.BASE_SCALE_FACTOR\n",
    "max_scale_factor          = seghead_config.UP_SCALE_FACTOR[0]\n",
    "num_upscale_layer_list    = [ int(log(i/max_scale_factor)/log(2)) for i in seghead_config.UP_SCALE_FACTOR ]\n",
    "lateral_layer_in_channels = BiFPNConfig.NUM_FEATURES\n",
    "seg_lateral_layers = nn.ModuleList([\n",
    "    # Might need to reverse the order (pay attention to the order in the bifpn output)\n",
    "    SegLateralLayer(in_channels       = lateral_layer_in_channels,\n",
    "                    out_channels      = seghead_config.OUT_CHANNELS,\n",
    "                    num_groups        = seghead_config.NUM_GROUPS,\n",
    "                    num_layers        = num_upscale_layers,\n",
    "                    base_scale_factor = base_scale_factor)\n",
    "    for num_upscale_layers in num_upscale_layer_list\n",
    "])\n",
    "\n",
    "head_segmask  = nn.Conv2d(in_channels  = seghead_config.OUT_CHANNELS,\n",
    "                                out_channels = seghead_config.NUM_CLASSES,\n",
    "                                kernel_size  = 1,\n",
    "                                padding      = 0,)\n",
    "\n",
    "if seghead_config.USES_LEARNED_UPSAMPLE:\n",
    "    head_upsample_layer = nn.ConvTranspose2d(in_channels  = seghead_config.NUM_CLASSES,\n",
    "                                                    out_channels = seghead_config.NUM_CLASSES,\n",
    "                                                    kernel_size  = 6,\n",
    "                                                    stride       = 4,\n",
    "                                                    padding      = 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse feature maps at each resolution (from low res to high res)...\n",
    "for idx, (lateral_layer, bifpn_output) in enumerate(zip(seg_lateral_layers[::-1], bifpn_output_list[::-1])):\n",
    "    fmap = lateral_layer(bifpn_output)\n",
    "\n",
    "    if idx == 0:\n",
    "        fmap_acc  = fmap\n",
    "    else:\n",
    "        fmap_acc += fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 256, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmap_acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction...\n",
    "pred_map = head_segmask(fmap_acc)\n",
    "\n",
    "# Upscale...\n",
    "max_scale_factor = seghead_config.UP_SCALE_FACTOR[0]\n",
    "pred_map = F.interpolate(pred_map,\n",
    "                            scale_factor  = max_scale_factor,\n",
    "                            mode          = 'bilinear',\n",
    "                            align_corners = False)                   \\\n",
    "            if not seghead_config.USES_LEARNED_UPSAMPLE else \\\n",
    "            head_upsample_layer(pred_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 1024, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_map.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana-4.0.58-py3-ml",
   "language": "python",
   "name": "ana-4.0.58-py3-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
