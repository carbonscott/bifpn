{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import exploratory tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ml_utils.misc import set_seed, print_layers, get_device\n",
    "from ml_utils.data import Pad, Resize, generate_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class ConvNextV2BackBone(nn.Module):\n",
    "    def __init__(self, pretrained = True):\n",
    "        super().__init__()\n",
    "\n",
    "        model = timm.create_model('convnextv2_atto.fcmae', pretrained=pretrained)\n",
    "\n",
    "        num_channels = 1\n",
    "\n",
    "        ave_weight_patch_embd = model.stem[0].weight.data.mean(dim = num_channels, keepdim = True) # [40, 3, 4, 4] -> [40, 1, 4, 4]\n",
    "        model.stem[0] = nn.Conv2d(num_channels, 40, kernel_size=(4, 4), stride=(4, 4))\n",
    "        model.stem[0].weight.data = ave_weight_patch_embd\n",
    "        \n",
    "        self.stem  = model.stem\n",
    "        self.stages = model.stages\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input through embeddings\n",
    "        embedding_output = self.stem(x)\n",
    "\n",
    "        # Initialize a list to hold the feature maps from each stage\n",
    "        stage_feature_maps = []\n",
    "\n",
    "        # Manually forward through each stage\n",
    "        hidden_states = embedding_output\n",
    "        for stage in self.stages:\n",
    "            hidden_states = stage(hidden_states)\n",
    "            stage_feature_maps.append(hidden_states)\n",
    "\n",
    "        return embedding_output, stage_feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "model = ConvNextV2BackBone(True).to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bifpn.bifpn        import BiFPN\n",
    "from bifpn.bifpn_config import BiFPNConfig\n",
    "from bifpn.utils_build  import BackboneToBiFPNAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B, C, H, W = 10, 1, 1920, 1920\n",
      "batch_input = np.random.rand(B, C, H, W)\n",
      "\n",
      "H_unify, W_unify = 1024, 1024\n",
      "resizer = Resize(H_unify, W_unify)\n",
      "\n",
      "batch_input_unify = resizer(batch_input.reshape(B*C, H, W)).reshape(B, C, H_unify, W_unify)\n",
      "batch_input_unify_tensor = torch.from_numpy(batch_input_unify).to(torch.float)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 10, 1, 1920, 1920\n",
    "batch_input = np.random.rand(B, C, H, W)\n",
    "\n",
    "H_unify, W_unify = 1024, 1024\n",
    "resizer = Resize(H_unify, W_unify)\n",
    "\n",
    "batch_input_unify = resizer(batch_input.reshape(B*C, H, W)).reshape(B, C, H_unify, W_unify)\n",
    "batch_input_unify_tensor = torch.from_numpy(batch_input_unify).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_input_unify_tensor_cuda = batch_input_unify_tensor.to(device)\n",
    "    embedding_output, stage_feature_maps = model(batch_input_unify_tensor_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 40, 256, 256])\n",
      "torch.Size([10, 80, 128, 128])\n",
      "torch.Size([10, 160, 64, 64])\n",
      "torch.Size([10, 320, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(stage_feature_maps)):\n",
    "    print(stage_feature_maps[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bifpn_features = 256\n",
    "backbone_output_channels = {\n",
    "        \"layer1\" : 40,\n",
    "        \"layer2\" : 80,\n",
    "        \"layer3\" : 160,\n",
    "        \"layer4\" : 320,\n",
    "}\n",
    "backbone_to_bifpn = BackboneToBiFPNAdapter(num_bifpn_features, backbone_output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bifpn_input_list = backbone_to_bifpn(stage_feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256, 256, 256])\n",
      "torch.Size([10, 256, 128, 128])\n",
      "torch.Size([10, 256, 64, 64])\n",
      "torch.Size([10, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bifpn_input_list)):\n",
    "    print(bifpn_input_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiFPNConfig = BiFPN.get_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiFPNConfig(RELU_INPLACE=False, DOWN_SCALE_FACTOR=0.5, UP_SCALE_FACTOR=2, NUM_BLOCKS=1, NUM_FEATURES=256, NUM_LEVELS=4, BASE_LEVEL=2, BN=BNConfig(EPS=1e-05, MOMENTUM=0.1), FUSION=FusionConfig(EPS=1e-05))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BiFPNConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the BiFPN layer...\n",
    "bifpn = BiFPN(config = BiFPNConfig)\n",
    "bifpn_output_list = bifpn(bifpn_input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256, 256, 256])\n",
      "torch.Size([10, 256, 128, 128])\n",
      "torch.Size([10, 256, 64, 64])\n",
      "torch.Size([10, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bifpn_output_list)):\n",
    "    print(bifpn_output_list[i].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana-4.0.58-py3-ml",
   "language": "python",
   "name": "ana-4.0.58-py3-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
